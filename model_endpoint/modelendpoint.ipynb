{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faith/pythonenvs/model_endpoint/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/bart-large-cnn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pythonenvs/model_endpoint/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/pythonenvs/model_endpoint/lib/python3.11/site-packages/transformers/modeling_utils.py:4245\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4235\u001b[0m         load_contexts\u001b[38;5;241m.\u001b[39mappend(tp_device)\n\u001b[1;32m   4237\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[1;32m   4238\u001b[0m         (\n\u001b[1;32m   4239\u001b[0m             model,\n\u001b[1;32m   4240\u001b[0m             missing_keys,\n\u001b[1;32m   4241\u001b[0m             unexpected_keys,\n\u001b[1;32m   4242\u001b[0m             mismatched_keys,\n\u001b[1;32m   4243\u001b[0m             offload_index,\n\u001b[1;32m   4244\u001b[0m             error_msgs,\n\u001b[0;32m-> 4245\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4246\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4252\u001b[0m \u001b[43m            \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4253\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4256\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4257\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4258\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4259\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4260\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4261\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4262\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4263\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4265\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4266\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/pythonenvs/model_endpoint/lib/python3.11/site-packages/transformers/modeling_utils.py:4622\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4620\u001b[0m             model\u001b[38;5;241m.\u001b[39mapply(model\u001b[38;5;241m.\u001b[39m_initialize_weights)\n\u001b[1;32m   4621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4622\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4624\u001b[0m \u001b[38;5;66;03m# Set some modules to fp32 if any\u001b[39;00m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_in_fp32_modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/pythonenvs/model_endpoint/lib/python3.11/site-packages/torch/nn/modules/module.py:1032\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \n\u001b[1;32m    998\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \n\u001b[1;32m   1030\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1032\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/pythonenvs/model_endpoint/lib/python3.11/site-packages/torch/nn/modules/module.py:1032\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \n\u001b[1;32m    998\u001b[0m \u001b[38;5;124;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \n\u001b[1;32m   1030\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1032\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/pythonenvs/model_endpoint/lib/python3.11/site-packages/torch/nn/modules/module.py:1033\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m   1032\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m-> 1033\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/pythonenvs/model_endpoint/lib/python3.11/site-packages/transformers/modeling_utils.py:1853\u001b[0m, in \u001b[0;36mPreTrainedModel._initialize_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hf_initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1854\u001b[0m module\u001b[38;5;241m.\u001b[39m_is_hf_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/pythonenvs/model_endpoint/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:760\u001b[0m, in \u001b[0;36mBartPreTrainedModel._init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    758\u001b[0m         module\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mEmbedding):\n\u001b[0;32m--> 760\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m         module\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata[module\u001b[38;5;241m.\u001b[39mpadding_idx]\u001b[38;5;241m.\u001b[39mzero_()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faith/pythonenvs/model_endpoint/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bucketresearch/politicalBiasBERT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bucketresearch/politicalBiasBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./models/bucketresearch/politicalBiasBERT\")\n",
    "model.save_pretrained(\"./models/bucketresearch/politicalBiasBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"With Donald Trump’s return to the White House, the long-held conservative grudge against affirmative action and programs designed to upend the effects of racial discrimination has transformed into a witch hunt. In the past decade, conservatives have cycled through attacks on wokeness, affirmative action, critical race theory, and the diversity-equity-and-inclusion initiatives known, now pejoratively, as D.E.I.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda:0')\n",
    "labels = torch.tensor([0])\n",
    "labels = labels.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.684238612651825, 0.24242424964904785, 0.07333706319332123]\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs, labels=labels)\n",
    "loss, logits = outputs[:2]\n",
    "\n",
    "# [0] -> left \n",
    "# [1] -> center\n",
    "# [2] -> right\n",
    "print(logits.softmax(dim=-1)[0].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"newsmediabias/Roberta_Sentiment_Classification\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"newsmediabias/Roberta_Sentiment_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_pretrained(\"./models/newsmediabias/Roberta_Sentiment_Classification\")\n",
    "# model.save_pretrained(\"./models/newsmediabias/Roberta_Sentiment_Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"With Donald Trump’s return to the White House, the long-held conservative grudge against affirmative action and programs designed to upend the effects of racial discrimination has transformed into a witch hunt. In the past decade, conservatives have cycled through attacks on wokeness, affirmative action, critical race theory, and the diversity-equity-and-inclusion initiatives known, now pejoratively, as D.E.I.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You know OgthelDim, if you substitute Trump's name for Trudope's the sentence is still applicable. \\\\n\\\\nAgreed, we are talking about 100 people in a Google workforce of  54,000.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\",return_overflowing_tokens=True, stride=6, max_length=512)\n",
    "inputs = inputs.to('cuda:0')\n",
    "labels = torch.tensor([0])\n",
    "labels = labels.to('cuda:0')\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss, logits = outputs[:2]\n",
    "\n",
    "# [0] -> positive\n",
    "# [1] -> neutral\n",
    "# [2] -> negative\n",
    "print(logits.softmax(dim=-1)[0].tolist()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3798"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08200571686029434, 0.32611462473869324, 0.591879665851593]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True,padding=True)\n",
    "inputs = inputs.to('cuda:0')\n",
    "labels = torch.tensor([0])\n",
    "labels = labels.to('cuda:0')\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss, logits = outputs[:2]\n",
    "\n",
    "# [0] -> positive\n",
    "# [1] -> neutral\n",
    "# [2] -> negative\n",
    "print(logits.softmax(dim=-1)[0].tolist()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faith/pythonenvs/model_endpoint/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig,AutoTokenizer,BertForTokenClassification\n",
    "import math\n",
    "model_path=\"tim1900/bert-chunker-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=255,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "device = 'cuda'# or cuda\n",
    "model = BertForTokenClassification.from_pretrained(model_path, ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./models/tim1900/bert-chunker-2\")\n",
    "model.save_pretrained(\"./models/tim1900/bert-chunker-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_text(model,text:str, tokenizer, prob_threshold=0.5)->list[str]:\n",
    "    # slide context window chunking\n",
    "    MAX_TOKENS=191\n",
    "    tokens=tokenizer(text, return_tensors=\"pt\",truncation=False)\n",
    "    input_ids=tokens['input_ids']\n",
    "    attention_mask=tokens['attention_mask'][:,0:MAX_TOKENS]\n",
    "    attention_mask=attention_mask.to(model.device)\n",
    "    CLS=input_ids[:,0].unsqueeze(0)\n",
    "    SEP=input_ids[:,-1].unsqueeze(0)\n",
    "    input_ids=input_ids[:,1:-1]\n",
    "    model.eval()\n",
    "    split_str_poses=[]\n",
    "    \n",
    "    token_pos = []\n",
    "\n",
    "    windows_start =0\n",
    "    windows_end= 0\n",
    "    logits_threshold = math.log(1/prob_threshold-1)\n",
    "    \n",
    "    print(f'Processing {input_ids.shape[1]} tokens...')\n",
    "    while windows_end <= input_ids.shape[1]:\n",
    "        windows_end= windows_start + MAX_TOKENS-2\n",
    "\n",
    "        ids=torch.cat((CLS, input_ids[:,windows_start:windows_end],SEP),1)\n",
    "\n",
    "        ids=ids.to(model.device)\n",
    "        \n",
    "        output=model(input_ids=ids,attention_mask=torch.ones(1, ids.shape[1],device=model.device))\n",
    "        logits = output['logits'][:, 1:-1,:]\n",
    "        chunk_decision = (logits[:,:,1]>(logits[:,:,0]-logits_threshold))\n",
    "        greater_rows_indices = torch.where(chunk_decision)[1].tolist()\n",
    "\n",
    "        # null or not\n",
    "        if len(greater_rows_indices)>0 and (not (greater_rows_indices[0] == 0 and len(greater_rows_indices)==1)):\n",
    "\n",
    "            split_str_pos=[tokens.token_to_chars(sp + windows_start + 1).start for sp in greater_rows_indices]\n",
    "            token_pos +=[sp + windows_start + 1 for sp in greater_rows_indices]\n",
    "            split_str_poses += split_str_pos\n",
    "\n",
    "            windows_start = greater_rows_indices[-1] + windows_start\n",
    "\n",
    "        else:\n",
    "\n",
    "            windows_start = windows_end\n",
    "\n",
    "    substrings = [text[i:j] for i, j in zip([0] + split_str_poses, split_str_poses+[len(text)])]\n",
    "    token_pos = [0] + token_pos\n",
    "    return substrings,token_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 743 tokens...\n",
      "-----chunk: 0----token_idx: 0--------\n",
      "LONDON — Dozens of countries, including Germany, South Africa and Mexico, said Friday that President Donald Trump’s decision to sanction the International Criminal Court would “erode the international rule of law.”\n",
      "\n",
      "\n",
      "-----chunk: 1----token_idx: 42--------\n",
      "The joint statement by 79 countries came hours after Trump signed an executive order slapping financial sanctions and visa restrictions against ICC staff and their family members, alleging the court has improperly targeted the United States and Israel.\n",
      "\n",
      "\n",
      "-----chunk: 2----token_idx: 83--------\n",
      "“Such measures increase the risk of impunity for the most serious crimes and threaten to erode the international rule of law, which is crucial for promoting global order and security,” the 79 countries, including Canada and France, said in a statement publicly released by numerous governments.\n",
      "\n",
      "\n",
      "-----chunk: 3----token_idx: 139--------\n",
      "The statement added that “sanctions could jeopardize the confidentiality of sensitive information and the safety of those involved—including victims, witnesses, and court officials, many of whom are our nationals.”\n",
      "\n",
      "\n",
      "-----chunk: 4----token_idx: 180--------\n",
      "The International Criminal Court in The Hague, Netherlands. \n",
      "-----chunk: 5----token_idx: 190--------\n",
      "Alex Gottschalk / DeFodi Images via Getty Images\n",
      "\n",
      "\n",
      "-----chunk: 6----token_idx: 204--------\n",
      "The signatories said the sanctions may result in the ICC having to close its field offices.\n",
      "\n",
      "\n",
      "-----chunk: 7----token_idx: 223--------\n",
      "\"We regret any attempts to undermine the court’s independence, integrity and impartiality,\" they said, crediting the \"ICC’s indispensable role in ending impunity, promoting the rule of law, and fostering lasting respect for international law and human rights.\"\n",
      "\n",
      "\n",
      "-----chunk: 8----token_idx: 284--------\n",
      "The United States and Israel are among a minority of around 40 countries that never signed up to the ICC, an international court based in the Netherlands that seeks to hold to account the perpetrators of war crimes, like genocide.\n",
      "\n",
      "\n",
      "-----chunk: 9----token_idx: 327--------\n",
      "But after some historic cooperation between Washington and the ICC, Trump's executive order Thursday accused the world body of “illegitimate and baseless actions targeting America and our close ally Israel.”\n",
      "\n",
      "\n",
      "-----chunk: 10----token_idx: 364--------\n",
      "In November, the ICC issued arrest warrants for Israeli Prime Minister Benjamin Netanyahu and former Israeli Defense Minister Yoav Gallant, as well as for Hamas leaders Yahya Sinwar, Mohammad Deif and Ismail Haniyeh.\n",
      "\n",
      "\n",
      "-----chunk: 11----token_idx: 410--------\n",
      "The warrants relate to events on and since Oct. 7, 2023, when Hamas-led terrorist attacks killed 1,200 people and saw around 250 others taken hostage, according to Israeli officials. Since, then Israel has launched a military offensive that has killed more than 47,500 people in the Gaza Strip, according to local health officials.\n",
      "\n",
      "\n",
      "-----chunk: 12----token_idx: 479--------\n",
      "The court said there was reason to believe Netanyahu and Gallant used “starvation as a method of warfare” by restricting humanitarian aid and intentionally targeting civilians in Israel’s campaign in Gaza. Israel, which also does not recognize the ICC, dismissed those charges as false and antisemitic.\n",
      "\n",
      "\n",
      "-----chunk: 13----token_idx: 538--------\n",
      "The court's \"recent actions against Israel and the United States set a dangerous precedent, directly endangering current and former United States personnel, including active service members of the Armed Forces, by exposing them to harassment, abuse, and possible arrest,\" the executive order said.\n",
      "\n",
      "\n",
      "-----chunk: 14----token_idx: 594--------\n",
      "Its signing appeared timed to coincide with Netanyahu's visit to Washington, in which Trump made the surprise announcement that he wanted the U.S. to take control of the Gaza Strip, shocking and outraging many officials, activists and experts around the world.\n",
      "\n",
      "\n",
      "-----chunk: 15----token_idx: 648--------\n",
      "Washington’s historical relationship with the ICC is a complex one.\n",
      "\n",
      "\n",
      "-----chunk: 16----token_idx: 661--------\n",
      "The administration of President Bill Clinton was involved in negotiating the 1998 Rome Statute on which the ICC is based. \n",
      "-----chunk: 17----token_idx: 682--------\n",
      "But the U.S. opposed the final draft because of fears it “could subject U.S. soldiers and officials to politicized prosecutions,” according to the Council on Foreign Relations.\n",
      "\n",
      "\n",
      "-----chunk: 18----token_idx: 722--------\n",
      "Clinton later signed the statute but asked it not be sent to the Senate for ratification until these concerns were addressed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # text to be chunked, can be any size.\n",
    "# text='''In the heart of the bustling city, where towering skyscrapers touch the clouds and the symphony \n",
    "#     of honking cars never ceases, Sarah, an aspiring novelist, found solace in the quiet corners of the ancient library \n",
    "#     Surrounded by shelves that whispered stories of centuries past, she crafted her own world with words, oblivious to the rush outside Dr.Alexander Thompson, aboard the spaceship 'Pandora's Venture', was en route to the newly discovered exoplanet Zephyr-7. \n",
    "#     As the lead astrobiologist of the expedition, his mission was to uncover signs of microbial life within the planet's subterranean ice caves. \n",
    "#     With each passing light year, the anticipation of unraveling secrets that could alter humanity's\n",
    "#      understanding of life in the universe grew ever stronger.'''\n",
    "import json\n",
    "\n",
    "with open('File1.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "text = data['text']\n",
    "text = \"LONDON — Dozens of countries, including Germany, South Africa and Mexico, said Friday that President Donald Trump’s decision to sanction the International Criminal Court would “erode the international rule of law.”\\n\\nThe joint statement by 79 countries came hours after Trump signed an executive order slapping financial sanctions and visa restrictions against ICC staff and their family members, alleging the court has improperly targeted the United States and Israel.\\n\\n“Such measures increase the risk of impunity for the most serious crimes and threaten to erode the international rule of law, which is crucial for promoting global order and security,” the 79 countries, including Canada and France, said in a statement publicly released by numerous governments.\\n\\nThe statement added that “sanctions could jeopardize the confidentiality of sensitive information and the safety of those involved—including victims, witnesses, and court officials, many of whom are our nationals.”\\n\\nThe International Criminal Court in The Hague, Netherlands. Alex Gottschalk / DeFodi Images via Getty Images\\n\\nThe signatories said the sanctions may result in the ICC having to close its field offices.\\n\\n\\\"We regret any attempts to undermine the court’s independence, integrity and impartiality,\\\" they said, crediting the \\\"ICC’s indispensable role in ending impunity, promoting the rule of law, and fostering lasting respect for international law and human rights.\\\"\\n\\nThe United States and Israel are among a minority of around 40 countries that never signed up to the ICC, an international court based in the Netherlands that seeks to hold to account the perpetrators of war crimes, like genocide.\\n\\nBut after some historic cooperation between Washington and the ICC, Trump's executive order Thursday accused the world body of “illegitimate and baseless actions targeting America and our close ally Israel.”\\n\\nIn November, the ICC issued arrest warrants for Israeli Prime Minister Benjamin Netanyahu and former Israeli Defense Minister Yoav Gallant, as well as for Hamas leaders Yahya Sinwar, Mohammad Deif and Ismail Haniyeh.\\n\\nThe warrants relate to events on and since Oct. 7, 2023, when Hamas-led terrorist attacks killed 1,200 people and saw around 250 others taken hostage, according to Israeli officials. Since, then Israel has launched a military offensive that has killed more than 47,500 people in the Gaza Strip, according to local health officials.\\n\\nThe court said there was reason to believe Netanyahu and Gallant used “starvation as a method of warfare” by restricting humanitarian aid and intentionally targeting civilians in Israel’s campaign in Gaza. Israel, which also does not recognize the ICC, dismissed those charges as false and antisemitic.\\n\\nThe court's \\\"recent actions against Israel and the United States set a dangerous precedent, directly endangering current and former United States personnel, including active service members of the Armed Forces, by exposing them to harassment, abuse, and possible arrest,\\\" the executive order said.\\n\\nIts signing appeared timed to coincide with Netanyahu's visit to Washington, in which Trump made the surprise announcement that he wanted the U.S. to take control of the Gaza Strip, shocking and outraging many officials, activists and experts around the world.\\n\\nWashington’s historical relationship with the ICC is a complex one.\\n\\nThe administration of President Bill Clinton was involved in negotiating the 1998 Rome Statute on which the ICC is based. But the U.S. opposed the final draft because of fears it “could subject U.S. soldiers and officials to politicized prosecutions,” according to the Council on Foreign Relations.\\n\\nClinton later signed the statute but asked it not be sent to the Senate for ratification until these concerns were addressed.\"\n",
    "\n",
    "\n",
    "# chunk the text. The prob_threshold should be between (0, 1). The lower it is, the more chunks will be generated.\n",
    "chunks, token_pos=chunk_text(model,text, tokenizer, prob_threshold=0.5)\n",
    "\n",
    "# print chunks\n",
    "for i, (c,t) in enumerate(zip(chunks,token_pos)):\n",
    "    print(f'-----chunk: {i}----token_idx: {t}--------')\n",
    "    print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n",
      "254\n",
      "296\n",
      "216\n",
      "60\n",
      "50\n",
      "93\n",
      "262\n",
      "232\n",
      "209\n",
      "218\n",
      "333\n",
      "304\n",
      "299\n",
      "262\n",
      "69\n",
      "122\n",
      "178\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_endpoint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
